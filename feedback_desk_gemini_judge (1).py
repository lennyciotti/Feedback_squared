# -*- coding: utf-8 -*-
"""Feedback Desk Gemini Judge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bJmmx0zDXJ4Dx-QZMURWzf-UR0cqxD02

**Project Overview:**

This notebook implements an automated pipeline to evaluate the quality of feedback generated by the "Feedback Desk" product. We utilize a Large Language Model (LLM) acting as an expert judge to assess student essays and the corresponding feedback based on specific criteria.

**Workflow:**

Load Data: Import student essays and generated feedback from pickle (.pkl) files stored in Google Drive.

Merge & Clean: Combine the datasets into a unified format for evaluation.

LLM Evaluation: Iteratively send each essay-feedback pair to the LLM (Gemini here) with a strict rubric.

Save Results: Store the quantitative scores and qualitative justifications back to Google Drive for analysis.
"""

import os
import pandas as pd
import json
from dotenv import load_dotenv
from typing import Dict, Any
from tqdm import tqdm
!pip install -q -U google-generativeai
import google.generativeai as genai
from google.colab import userdata

api_key = userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=api_key)
print("‚úÖ Gemini API Key loaded")

if api_key:
    print("‚úÖ Gemini API key is loaded!")
else:
    print("‚ùå Gemini API key NOT loaded.")

SYSTEM_ROLE_JUDGE = """
You are a judge that evaluates feedback on written assignments.
You are judging the feedback's {tone}, its level of {detail}, and how appropriate the feedback is around {grammar}, {structure} as well as {content}.
You will receive both the original essay and the feedback.
The feedback is broken into two sections, high level feedback on the entire assignment and comments
which contains tone, level of detail, grammar, structure and content.
For each assignment you will one score for each dimension of feedback.
Your evaluation must strictly adhere to the provided dimensions.
Your response must be and can only be a JSON object, containing no introductory text outside the JSON format (such as ‚ÄúHere is the evaluation...‚Äù).
"""

EVALUATION_RUBRIC = """
Please rate and evaluate this feedback according to the following criteria.
[Evaluation Criteria]
1.  Tone[1-5 points]: For tone you will consider the high level feedback and the comments and judge if the feedback's tone is
    4 = Very positive and encouraging
    3 = Somewhat positive and encouraging
    2 = Not really positive and encouraging
    1 = Not at all positive and encouraging
2.  Level of detail [1-5 points]: For the level of detail you will consider the high level feedback and the comments and judge if the feedback:
    4 = Very detailed, specific and actionable
    3 = Somewhat detailed, specific and actionable
    2 = Not really detailed, specific and actionable
    1 = Not at all detailed specific or actionable
3.  Grammar [1-5 points]: For grammar you will consider both the original essay and the high level feedback and comments and judge if the grammar feedback:
    4 = Addressed all grammar and mechanics issues in the original essay
    3 = Addressed most grammar and mechanics issues in the original essay
    2 = Addressed some grammar and mechanics issues in the original essay
    1 = Addressed none of the grammar and mechanics issues in the original essay
4.  Stucture [1-5 points]: For stucture you will consider both the original essay and the high level feedback and comments and judge if the stucture feedback:
    4 = Addressed all stucture and mechanics issues in the original essay
    3 = Addressed most stucture and mechanics issues in the original essay
    2 = Addressed some stucture and mechanics issues in the original essay
    1 = Addressed none of the stucture and mechanics issues in the original essay
5.  Content [1-5 points]: For content you will consider both the original essay and the high level feedback and comments and judge if the content feedback:
    4 = Addressed all content and mechanics issues in the original essay
    3 = Addressed most content and mechanics issues in the original essay
    2 = Addressed some content and mechanics issues in the original essay
    1 = Addressed none of the content and mechanics issues in the original essay

[Output Format]
Please strictly adhere to the following JSON format when returning your evaluation results:
{
  "Tone": <score (int)>,
  "Level of detail": <score (int)>,
  "Grammar": <score (int)>,
  "Stucture": <score (int)>,
  "Content": "<score (int)>"
}
"""

"""Input student sample data and the Feedback Desk feedback data from google drive saved pkl file."""

from google.colab import drive
drive.mount('/content/drive')


# CONNECT TO THE SAMPLES DATABASE

# Define the path to your pickle file
pickle_file_path = '/content/drive/MyDrive/Practicum the Feedback Desk/Copy of SAMPLES.pkl'

# Unpickle the file into a DataFrame
samples_df = pd.read_pickle(pickle_file_path)

# Now 'df' is your unpickled pandas DataFrame
print(samples_df.head())

# CONNECT TO THE FEEDBACK RESULTS DATABASE

# Define the path to your pickle file
pickle_file_path = '/content/drive/MyDrive/Practicum the Feedback Desk/Copy of feedback_results_df.pkl'

# Unpickle the file into a DataFrame
results_df = pd.read_pickle(pickle_file_path)

# Now 'df' is your unpickled pandas DataFrame
print(results_df.head())

"""***2. Core Evaluation Logic***

This function acts as the bridge between our data and the LLM API.

Input: It takes the Student Essay and the Feedback Desk Output.

Prompt Engineering: It constructs a structured prompt combining the essay, feedback, and the grading rubric.

API Call: It sends the request to the model (forcing a JSON output for structured data) and handles any potential connection errors.
"""

gemini_model = genai.GenerativeModel("gemini-2.5-flash")

def get_llm_evaluation(essay: str, feedback: str, model: genai.GenerativeModel) -> Dict[str, Any] | None:
    """
    Construct the prompt and invoke the Gemini model to obtain the evaluation.
    """
# 1. Prompt Construction
    user_prompt = f"""
    [Original student essay]
    {essay}

    [Feedback Desk's feedback]
    {feedback}

    [Evaluation Criteria and Output Format]
    {EVALUATION_RUBRIC}

    IMPORTANT:
    You MUST return ONLY valid JSON.
    Do NOT include explanations, notes, or text outside JSON.
    """

    try:
        # 2. Configure generation parameters
        config = genai.types.GenerationConfig(
            temperature=0.2,
            response_mime_type="application/json"
        )

        # 3. Splicing System Role (Gemini can also be directly passed as context during generation)
        full_prompt = f"{SYSTEM_ROLE_JUDGE}\n\n{user_prompt}"

        # 4. Call the Gemini API
        response = model.generate_content(
            full_prompt,
            generation_config=config
        )

        # 5. Extract text
        output_text = response.text

        # 6. Parse JSON
        evaluation_data = json.loads(output_text)
        return evaluation_data

    except json.JSONDecodeError as e:
        print("‚ùå Error: Model did not return valid JSON.")
        # print(f"Raw model output:\n{output_text}")
        return None

    except Exception as e:
        print(f"‚ùå Error calling Gemini API: {e}")
        return None

"""***3. Data Processing & Batch Execution***

This is the main execution loop of the pipeline.

Data Loading: Reads the .pkl files using Pandas.

Data Merging: Joins the Samples and Results dataframes on essay_id to ensure we are evaluating the correct pairs.

Batch Processing: Iterates through every row (with a progress bar), cleans column names, sends data to the LLM, and collects the results.

Saving: Finally, exports the evaluation results to a new .pkl file in Google Drive.
"""

def main():
    # Define Path
    INPUT_PKL_PATH_SAMPLES = '/content/drive/MyDrive/Practicum the Feedback Desk/Copy of SAMPLES.pkl'
    INPUT_PKL_PATH_RESULTS = '/content/drive/MyDrive/Practicum the Feedback Desk/Copy of feedback_results_df.pkl'
    OUTPUT_PKL_PATH = '/content/drive/MyDrive/Practicum the Feedback Desk/Gemini_judges_evaluation_results.pkl'

    results_list = []

    # 1. load data
    try:
        print("Loading data...")
        samples_df = pd.read_pickle(INPUT_PKL_PATH_SAMPLES)
        results_df = pd.read_pickle(INPUT_PKL_PATH_RESULTS)

        samples_df['essay_id'] = samples_df['essay_id'].astype(str)
        results_df['essay_id'] = results_df['essay_id'].astype(str)

        # Merge Data
        # Use inner join to ensure only records with both original text and feedback are processed
        merged_data = pd.merge(samples_df, results_df, on='essay_id', how='inner')

        # Remove duplicates! Prevent duplicate rows from appearing in the original data.
        merged_data = merged_data.drop_duplicates(subset=['essay_id'])

        print(f"‚úÖ Data merged! Total unique essays to process: {len(merged_data)}")

        print(f"üîç Debug: Merged Columns: {merged_data.columns.tolist()}")

    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        return

# 2. Loop processing (using tqdm to display progress bar)
    print("üöÄ Starting Batch Evaluation...")

    for index, row in tqdm(merged_data.iterrows(), total=len(merged_data)):
        # Retrieve the ID (preferably essay_id)
        current_id = row.get('essay_id', row.get('sample_id', f'Unknown_{index}'))

        # Retrieve article content
        if 'essay' in row:
            current_essay = row['essay']
        elif 'essay_text' in row:
            current_essay = row['essay_text']
        # Handling potential suffix combinations
        elif 'essay_x' in row:
            current_essay = row['essay_x']
        elif 'essay_y' in row:
            current_essay = row['essay_y']
        else:
            # Only when none of the above names can be found should an error be reported.
            print(f"‚ö†Ô∏è Column missing for ID {current_id}: Check column names!")
            current_essay = ""

        # Get feedback (also check if high_level_feedback has a suffix)
        high_level = row.get('high_level_feedback', row.get('high_level_feedback_x', row.get('high_level_feedback_y', '')))
        comments = row.get('comments', row.get('comments_x', row.get('comments_y', '')))
        current_feedback = f"High Level Feedback:\n{high_level}\n\nSpecific Comments:\n{comments}"

        # Inspection Content
        if pd.isna(current_essay) or len(str(current_essay)) < 10:
            # Only when it's truly empty will it skip.
            print(f"‚ö†Ô∏è Skip ID {current_id}: Content empty.")
            continue

        # Call GPT (passing the current line's essay and feedback)
        evaluation = get_llm_evaluation(current_essay, current_feedback, gemini_model)

        if evaluation:
            evaluation['essay_id'] = current_id # Record the correct ID
            results_list.append(evaluation)
        else:
            # Failed to record
            results_list.append({
                "essay_id": current_id
            })

# 3. Save results
    final_df = pd.DataFrame(results_list)

    # Perform another deduplication to ensure absolute reliability.
    final_df = final_df.drop_duplicates(subset=['essay_id'])

    print(f"\nüíæ Saving {len(final_df)} unique results to Drive...")
    final_df.to_pickle(OUTPUT_PKL_PATH)

    print("üéâ Done! Preview:")
    print(final_df.head())

if __name__ == "__main__":
    main()