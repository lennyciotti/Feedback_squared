# -*- coding: utf-8 -*-
"""Feedback Desk GPT Judge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OBEKG20jqdekuAC8YrXi9C4Qh8uJOAu0
"""

import os
import sqlite3
import pandas as pd
import json
from openai import OpenAI
from dotenv import load_dotenv
from typing import Dict, Any

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if api_key:
    print("✅ OpenAI API key is loaded!")
else:
    print("❌ OpenAI API key NOT loaded.")

SYSTEM_ROLE_JUDGE = """
You are a judge that evaluates feedback on written assignments.
You are judging the feedback's {tone}, its level of {detail}, and how appropriate the feedback is around {grammar}, {structure} as well as {content}.
You will receive both the original essay and the feedback.
The feedback is broken into two sections, high level feedback on the entire assignment and comments
which contains tone, level of detail, grammar, structure and content.
For each assignment you will one score for each dimension of feedback.
Your evaluation must strictly adhere to the provided dimensions.
Your response must be and can only be a JSON object, containing no introductory text outside the JSON format (such as “Here is the evaluation...”).
"""

EVALUATION_RUBRIC = """
Please rate and evaluate this feedback according to the following criteria.
[Evaluation Criteria]
1.  Tone[1-5 points]: For tone you will consider the high level feedback and the comments and judge if the feedback's tone is
    4 = Very positive and encouraging
    3 = Somewhat positive and encouraging
    2 = Not really positive and encouraging
    1 = Not at all positive and encouraging
2.  Level of detail [1-5 points]: For the level of detail you will consider the high level feedback and the comments and judge if the feedback:
    4 = Very detailed, specific and actionable
    3 = Somewhat detailed, specific and actionable
    2 = Not really detailed, specific and actionable
    1 = Not at all detailed specific or actionable
3.  Grammar [1-5 points]: For grammar you will consider both the original essay and the high level feedback and comments and judge if the grammar feedback:
    4 = Addressed all grammar and mechanics issues in the original essay
    3 = Addressed most grammar and mechanics issues in the original essay
    2 = Addressed some grammar and mechanics issues in the original essay
    1 = Addressed none of the grammar and mechanics issues in the original essay
4.  Stucture [1-5 points]: For stucture you will consider both the original essay and the high level feedback and comments and judge if the stucture feedback:
    4 = Addressed all stucture and mechanics issues in the original essay
    3 = Addressed most stucture and mechanics issues in the original essay
    2 = Addressed some stucture and mechanics issues in the original essay
    1 = Addressed none of the stucture and mechanics issues in the original essay
5.  Content [1-5 points]: For content you will consider both the original essay and the high level feedback and comments and judge if the content feedback:
    4 = Addressed all content and mechanics issues in the original essay
    3 = Addressed most content and mechanics issues in the original essay
    2 = Addressed some content and mechanics issues in the original essay
    1 = Addressed none of the content and mechanics issues in the original essay

[Output Format]
Please strictly adhere to the following JSON format when returning your evaluation results:
{
  "Tone": <score (int)>,
  "Level of detail": <score (int)>,
  "Grammar": <score (int)>,
  "Stucture": <score (int)>,
  "Content": "<score (int)>"
}
"""

def load_data(samples_db_path: str, feedback_db_path: str) -> pd.DataFrame:
    """
    Load student papers and Feedback Desk feedback from two SQLite databases and merge them.
    """
    print(f"Loading data from {samples_db_path} and {feedback_db_path}...")
    try:
        conn_samples = sqlite3.connect(samples_db_path)
        essays_df = pd.read_sql_query("SELECT sample_id, essay_text FROM essays", conn_samples)
        conn_samples.close()

        conn_feedback = sqlite3.connect(feedback_db_path)
        feedback_df = pd.read_sql_query("SELECT sample_id, feedback_text FROM feedback", conn_feedback)
        conn_feedback.close()

        # Merge two DataFrames based on sample_id
        merged_df = pd.merge(essays_df, feedback_df, on="sample_id", how="inner")

        print(f"✅ Data loaded. Found {len(merged_df)} matching essays and feedback.")
        return merged_df

    except Exception as e:
        print(f"❌ Error loading data: {e}")
        print("Please ensure the .db file exists and that the table names (essays, feedback) and column names (sample_id, ...) are correct.")
        return pd.DataFrame() # Return an empty DataFrame

def save_results(results_df: pd.DataFrame, output_db_path: str):
    """
    Save the evaluation results to a new SQLite database file.
    """
    print(f"Saving {len(results_df)} results to {output_db_path}...")
    try:
        conn_output = sqlite3.connect(output_db_path)
        # ‘if_exists='replace’' will replace the old table. To append data, use ‘append’.
        results_df.to_sql('evaluations', conn_output, if_exists='replace', index=False)
        conn_output.close()
        print("✅ Results saved successfully.")
    except Exception as e:
        print(f"❌ Error saving results: {e}")

def get_llm_evaluation(essay: str, feedback: str, client: OpenAI) -> Dict[str, Any] | None:
    """
    Construct the prompt and invoke the GPT-4.0 model to obtain the evaluation.
    """
    # Build a structured user prompt
    user_prompt = f"""
    [Original student essay]
    {essay}

    [Feedback Desk's feedback]
    {feedback}

    [Evaluation Criteria and Output Format]
    {EVALUATION_RUBRIC}
    """

    try:
        response = client.responses.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_ROLE_JUDGE},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2
        )

        # Parsing the returned JSON string
        evaluation_json = response.choices[0].message.content
        evaluation_data = json.loads(evaluation_json)
        return evaluation_data

    except json.JSONDecodeError as e:
        print(f"❌ Error: GPT-4.0 未返回有效的JSON。 错误: {e}")
        print(f"原始回复: {evaluation_json}")
        return None
    except Exception as e:
        print(f"❌ Error calling OpenAI API: {e}")
        return None

def main():
    # Define Database File Path
    SAMPLES_DB_PATH = 'samples_data.db'
    FEEDBACK_DB_PATH = 'feedback_data.db'
    OUTPUT_DB_PATH = 'judges_data.sql' #The output file can be named .sql, but it remains a SQLite database file.

    # load data
    merged_data = load_data(SAMPLES_DB_PATH, FEEDBACK_DB_PATH)

    if merged_data.empty:
        print("No data to process; program exits.")
        return

    results_list = [] # Store all evaluation results

    # Iterate through each row of data (each paper and its feedback)
    for index, row in merged_data.iterrows():
        sample_id = row['sample_id']
        essay_text = row['essay_text']
        feedback_text = row['feedback_text']

        print(f"\n--- Evaluation Sample ID: {sample_id} ---")

        # Invoke GPT-4.0 for evaluation
        evaluation = get_llm_evaluation(essay_text, feedback_text, client)

        if evaluation:
            # Add the sample_id to the evaluation results for tracking purposes.
            evaluation['sample_id'] = sample_id
            results_list.append(evaluation)
            print(f"✅ Evaluation completed: {evaluation['justification']}")
        else:
            print(f"❌ Evaluation failed: Essay ID {sample_id}")
            # Record failed evaluations for subsequent retries.
            results_list.append({
                "sample_id": sample_id,
                "tone_score": None,
                "detail_score": None,
                "grammar_score": None,
                "stucture_score": None,
                "content_score": None
            })

    # Convert all result lists to DataFrames
    results_df = pd.DataFrame(results_list)

    # Reorder the columns so that sample_id is in the first column.
    if not results_df.empty:
        cols = ['sample_id'] + [col for col in results_df.columns if col != 'sample_id']
        results_df = results_df[cols]

    # Save the final result
    save_results(results_df, OUTPUT_DB_PATH)

    print("\n--- All evaluations have been completed and saved. ---")
    print(results_df.head())